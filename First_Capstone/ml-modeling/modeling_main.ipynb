{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Classification\n",
    "\n",
    "[Theory and Approach followed](#section1)\n",
    "- [Overfitting and Underfitting](#section2)\n",
    "- [Model evaluation criteria](#section3)\n",
    "\n",
    "[First (Baseline) Model](#section4)\n",
    "- [Import libraries](#section5)\n",
    "- [Split data into train test](#section6)\n",
    "- [Useful functions](#section7)\n",
    "- [Run and evaluate](#section8)\n",
    "\n",
    "[Dealing with class imbalance](#section9)\n",
    "- [Over and Under sampling](#section10)\n",
    "- [One vs Rest method after fixing class imbalance](#section11)\n",
    "- [Using class weights](#section12)\n",
    "\n",
    "[Other simple Models](#section13)\n",
    "\n",
    "[Bagging Models](#section14)\n",
    "\n",
    "[Boosting Models](#section14)\n",
    "\n",
    "[Stacking Models](#section14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Theory and Approach followed\n",
    "Our goal is to develop a model that describes the relationship between the features in the data and the target class (product category). So far we have got good understanding of the data and we can apply the machine learning algorithms and find out the best model. Technically speaking, this is Supervised Classification ML problem. Here we have the training data with the actual labels. We need to model that and use the model to predict labels for new data.\n",
    "\n",
    "We will start with a quick and dirty baseline model. And improve upon our model in interative fashion. Following are some ideas to keep in mind.\n",
    "\n",
    "<a id='section2'></a>\n",
    "### Overfitting and Underfitting\n",
    "When the model fits the training data too perfectly but not the test data, it is called overfitting. Such a model will not be useful for predictions. While at the other extreme end our model might fit the training data very poorly. Consequently, it can't fit test data any better and will give inaccurate predictions. Generally its a trade off between overfitting and underfitting. This is also called **Bias-Variance trade-off**. When the model fits too well to the training data (a subset of population) it has low Bias, that is, it mimics the data very well. But it might fail miserably in mimicking the test data (another subset of population). Or it has variance between different subsets of data. As we try to make this variance low, it will increase the Bias.\n",
    "    \n",
    "In our case, the number of data points is very large (67 thousand) as compared to the number of features. This will help in avoiding overfitting. We are going to always compare train and test accuracy to see how well the model fits. We should also design train-test split of the data wisely to regulate the bias-variance trade off.\n",
    "    \n",
    "<a id='section3'></a>    \n",
    "### Model Evaluation Criteria\n",
    "Evaluating our model closely relates with how the model is going to be used by the client. In case of classification problems one type of error (say false negative) might be more costly then another type of error (false positive). In such a case our model should be more stringent for false negatives.\n",
    "    \n",
    "In our case we need to classify each product (described by 93 features) into correct class. As such both type of errors have equal weightage. One important fact is that our traning data has class imbalance. There is more data for some classes then others. This is important consideration while chosing the model evaluation criterion. We should try and handle the class imbalance problem and see if improve our models. Criteria that we are going to use.\n",
    "- **Accuracy**: Although important we can't rely on just that. Because we have class imabalnce, our accuracy might be high but predictions for minority class might be wrong.\n",
    "\n",
    "- **Confusion Matirx**: This would tell us how many precdictions we got wrong for each class. And how well the minority classes are doing along with majority classes.\n",
    "\n",
    "- **Classification Matrix**: To measure precision, recall and F1 score (harmonic mean of precision and recall) for each class. It tells us average F1 score for all classes too. This can serve as a single value to compare the models.\n",
    "\n",
    "- **Log loss**: This is the evaluation criteria used by the client (Otto Group). So this is our main criterion to compare and improve the models. Each product has been labeled with one true category. For each product, we find a set of predicted probabilities (one for every category). The formula is then,\n",
    "  \\begin{equation*}\n",
    "        logloss=-\\dfrac{1}{N} \\sum_{i=1}^M \\sum_{i=1}^N y_{ij}log(p_{ij}),\n",
    "  \\end{equation*}\n",
    "    where N is the number of products in the test set, M is the number of class labels, log is the natural logarithm, yij is 1 if observation i is in class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Baseline Model\n",
    "\n",
    "<a id='section5'></a>\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all the necessary python libraries\n",
    "\n",
    "# basic computing and handling tabular data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from random import sample\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#statistics\n",
    "from scipy.stats import normaltest,skewtest,kurtosistest, spearmanr\n",
    "\n",
    "# machine learning \n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit,RandomizedSearchCV,cross_validate\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss,classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ensemble bagging boosting\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# saving to disk\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# over sampling for imbalanced classes\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, EditedNearestNeighbours, RepeatedEditedNearestNeighbours,InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# calculate weights for imbablanced classes\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to reproduce the results we can use a fixed random state\n",
    "# set global random seed\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.402093</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>0.149647</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>0.149647</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.402093</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>0.079240</td>\n",
       "      <td>13.508710</td>\n",
       "      <td>4.524667</td>\n",
       "      <td>4.665884</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>1.019683</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3          4         5         6  \\\n",
       "0  0.402093 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "1 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "2 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "3  0.402093 -0.210106 -0.307165  0.079240  13.508710  4.524667  4.665884   \n",
       "4 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "\n",
       "          7         8         9   ...          84        85        86  \\\n",
       "0 -0.293664 -0.291038 -0.243606   ...    0.246100 -0.420870 -0.249802   \n",
       "1  0.149647 -0.291038 -0.243606   ...   -0.280099 -0.420870 -0.249802   \n",
       "2  0.149647 -0.291038 -0.243606   ...   -0.280099 -0.420870 -0.249802   \n",
       "3 -0.293664 -0.291038  0.679472   ...   -0.280099 -0.047949  1.019683   \n",
       "4 -0.293664 -0.291038 -0.243606   ...    0.246100 -0.420870 -0.249802   \n",
       "\n",
       "         87        88        89        90        91        92  target  \n",
       "0 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "1 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "2 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "3 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "4 -0.413584 -0.299712  0.040798 -0.129516 -0.386938 -0.104963       1  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using normalized data for all models\n",
    "# using pca components instead of normal features is a good idea? I don't think so.\n",
    "# for logistic regression it is better to use nomralized data\n",
    "\n",
    "df = pd.read_csv(\"../data/train_norm.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "### Split the data into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.1,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# here the classes are imbalanced so we should use stratified split \n",
    "# the folds are made by preserving the percentage of samples for each class\n",
    "# note that the imbalance will still be their when we train the model using this split\n",
    "\n",
    "# since we have big amount of data our test set can be just 1% of all data\n",
    "# this will help in avoiding overfitting\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.1, random_state=42)\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [57972 30244  9427 ... 60232 28576 27516] TEST: [59081 21681 51999 ...  1777   269 53901]\n"
     ]
    }
   ],
   "source": [
    "# drop target column for features data X\n",
    "X = df.drop(\"target\",axis=1)\n",
    "y = df.target\n",
    "\n",
    "train_index = []\n",
    "test_index = []\n",
    "\n",
    "for tr, tes in sss.split(X,y):\n",
    "    print(\"TRAIN:\", tr, \"TEST:\", tes)\n",
    "    train_index = tr\n",
    "    test_index = tes\n",
    "\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of data sets\n",
      "X_train:  (55690, 93) y_train:  (55690,)\n",
      "X_test:  (6188, 93) y_test:  (6188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of data sets\")\n",
    "print(\"X_train: \", X_train.shape, \"y_train: \", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape,\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(fitted_model):\n",
    "    \"\"\"\n",
    "    Evaluate the given model.\n",
    "\n",
    "    Evaluate model using accuracy, confusion matrix, classification report and logloss for given model and test data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitted_model : dict\n",
    "        Dictionary with all the info related to the model \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Returns the fitted_model with added key values\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model = fitted_model.get(\"model\")\n",
    "    train_x = fitted_model.get(\"train_x\")\n",
    "    train_y = fitted_model.get(\"train_y\")\n",
    "    \n",
    "    test_x = fitted_model.get(\"test_x\")\n",
    "    test_y = fitted_model.get(\"test_y\")\n",
    "    \n",
    "    train_score = model.score(train_x,train_y).round(5)\n",
    "    test_score = model.score(test_x,test_y).round(5)\n",
    "    log_loss_train = log_loss(train_y, model.predict_proba(train_x)).round(5)\n",
    "    log_loss_test = log_loss(test_y, model.predict_proba(test_x)).round(5)\n",
    "    conf_matrix = confusion_matrix(test_y, model.predict(test_x))\n",
    "    classifi_report = classification_report(test_y, model.predict(test_x))\n",
    "    \n",
    "    print(\"Train score: \",train_score)\n",
    "    print(\"Test score: \",test_score)\n",
    "    print(\"Log loss train: \",log_loss_train)\n",
    "    print(\"Log loss test: \",log_loss_test)\n",
    "    print(\"\\nConfusion Matrix: \\n\", conf_matrix)\n",
    "    print(\"\\nClassification Report: \\n\", classifi_report)\n",
    "    \n",
    "    # update the model with all the evaluations\n",
    "    fitted_model.update({\"train_score\":train_score})\n",
    "    fitted_model.update({\"test_score\":test_score})\n",
    "    fitted_model.update({\"log_loss_train\":log_loss_train})\n",
    "    fitted_model.update({\"log_loss_test\":log_loss_test})\n",
    "    fitted_model.update({\"confusion_matrix\":conf_matrix})\n",
    "    fitted_model.update({\"classification_report\":classifi_report})    \n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_evaluate(model,main_params, hyper_params_grid, train_x, train_y, test_x, test_y, scoring,name=\"dummy\"):\n",
    "    \"\"\"\n",
    "    Fit the ML model\n",
    "\n",
    "    Fit the model using the params and hyper params\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : machine learning model\n",
    "    main_params: main parameters like learning rate etc. of the model\n",
    "    hyper_params_grid: hyper parameters grid to be used grid search to find params that best fit the model\n",
    "    scoring: scoring criterion to be used by grid search to find the best hyper params\n",
    "    name: to save the model to the disk\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Returns the fitted_model with all the information about model, train-test data, best hyper param etc.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # default scoring\n",
    "    scoring = \"neg_log_loss\"\n",
    "    random_seed = 42\n",
    "    \n",
    "    # baisc parameters\n",
    "    model.set_params(**main_params)\n",
    "    \n",
    "    # fit hyper params if provided\n",
    "    if(len(hyper_params_grid) >0):\n",
    "        \n",
    "        # grid search to find the best hyper param for given scoring\n",
    "        gsv = GridSearchCV(estimator=model,cv=3,param_grid=hyper_params_grid,scoring=scoring)\n",
    "        gsv.fit(train_x,train_y)\n",
    "                \n",
    "        best_params = gsv.best_params_\n",
    "        main_params.update(best_params)        \n",
    "        print(\"Best params\", best_params)\n",
    "        \n",
    "        # choose the best estimator as the model\n",
    "        model = gsv.best_estimator_\n",
    "        \n",
    "    else:\n",
    "        # fit with fixed hyper params\n",
    "        model.fit(train_x,train_y)\n",
    "        \n",
    "    print(\"\\nAll params: \\n\", main_params)\n",
    "    print(\"\\nClassifier model: \\n\", model)\n",
    "        \n",
    "    # dict to save detailed info of the model    \n",
    "    fitted_model = {}\n",
    "    fitted_model.update({\"model\":model})\n",
    "    fitted_model.update({\"train_x\":train_x})\n",
    "    fitted_model.update({\"train_y\":train_y})\n",
    "    fitted_model.update({\"test_x\":test_x})\n",
    "    fitted_model.update({\"test_y\":test_y})\n",
    "    \n",
    "    fitted_model = evaluate_model(fitted_model)\n",
    "    \n",
    "    # save to the disk\n",
    "    joblib.dump(fitted_model,filename=\"../models-repo/\"+name+\".sav\")\n",
    "    \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section'></a>\n",
    "### Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base line model - Logistic Regression\n",
    "# Try out different params and hyper params and save the results\n",
    "\n",
    "# last two not done yet\n",
    "main_params_list = {\"l1_liblinear\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l1\",\"solver\":\"liblinear\"},\n",
    "                    \"l1_saga\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l1\",\"solver\":\"saga\"},\n",
    "                    \"l1_saga_multinomial\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l1\",\"solver\":\"saga\",\"multi_class\":\"multinomial\"},\n",
    "                    \"l2_lbfgs\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l2\",\"solver\":\"lbfgs\"},\n",
    "                    \"l2_lbfgs_multinomial\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l2\",\"solver\":\"lbfgs\",\"multi_class\":\"multinomial\"},\n",
    "                    #\"l2_sag\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l2\",\"solver\":\"sag\"},\n",
    "                    #\"l2_sag_multinomial\": {\"cv\":3,\"random_state\":42,\"penalty\":\"l2\",\"solver\":\"sag\",\"multi_class\":\"multinomial\"}                    \n",
    "                   }\n",
    "\n",
    "hyper_params_grid = {\"Cs\":[1,10,100]}\n",
    "\n",
    "lrcv = LogisticRegressionCV()\n",
    "\n",
    "# temporay commenting out\n",
    "# for name, main_params in main_params_list.items():\n",
    "#     fit_evaluate(lrcv, main_params, hyper_params_grid, \\\n",
    "#                  X_train, y_train,X_test, y_test,\"neg_log_loss\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Base line model - Logistic Regression\n",
    "# Try out different params and hyper params and save the results\n",
    "\n",
    "# last two not done yet\n",
    "main_params_list = {\"l1_liblinear\": {\"penalty\":\"l1\",\"solver\":\"liblinear\"},\n",
    "                    \"l1_saga\": {\"penalty\":\"l1\",\"solver\":\"saga\"},\n",
    "                    \"l1_saga_multinomial\": {\"penalty\":\"l1\",\"solver\":\"saga\",\"multi_class\":\"multinomial\"},\n",
    "                    \"l2_lbfgs\": {\"penalty\":\"l2\",\"solver\":\"lbfgs\",\"max_iter\":200},\n",
    "                    #\"l2_sag\": {\"penalty\":\"l2\",\"solver\":\"sag\",\"max_iter\":1000},\n",
    "                    \"l2_lbfgs_multinomial\": {\"penalty\":\"l2\",\"solver\":\"lbfgs\",\"multi_class\":\"multinomial\",\"max_iter\":1000},\n",
    "                    #\"l2_sag_multinomial\": {\"penalty\":\"l2\",\"solver\":\"sag\",\"multi_class\":\"multinomial\",\"max_iter\":4000}                    \n",
    "                   }\n",
    "\n",
    "hyper_params_grid = {\"Cs\":[10,100]}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# temporay commenting out\n",
    "# for name, main_params in main_params_list.items():\n",
    "#     fit_evaluate(lrcv, main_params, hyper_params_grid, \\\n",
    "#                  X_train, y_train,X_test, y_test,\"neg_log_loss\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the saved models\n",
    "model_names = ['l1_liblinear','l1_saga','l1_saga_multinomial','l2_lbfgs','l2_lbfgs_multinomial']\n",
    "models = [joblib.load(\"../models-repo/\" + name + \".sav\") for name in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'train_x', 'train_y', 'test_x', 'test_y', 'train_score', 'test_score', 'log_loss_train', 'log_loss_test', 'confusion_matrix', 'classification_report'])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the keys of the model objects\n",
    "models[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract important params from all params\n",
    "def get_imp_params(params):    \n",
    "    param_str = \"C:\"+str(params.get(\"Cs\"))+\", \" \\\n",
    "            + params.get(\"solver\") + \", \" + params.get(\"penalty\") + \", \" + params.get(\"multi_class\")\n",
    "    return param_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      " LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "           refit=True, scoring=None, solver='liblinear', tol=0.0001,\n",
      "           verbose=0)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  56   27    0    0    0   21    2   41   46]\n",
      " [   0 1440  138    5    6    6   10    5    2]\n",
      " [   0  566  217    2    0    1   10    3    1]\n",
      " [   0  169   29   44    4   19    4    0    0]\n",
      " [   0   13    2    0  259    0    0    0    0]\n",
      " [   3   34    1    3    1 1302   23   28   19]\n",
      " [   2   54   20    1    1   30  159   14    3]\n",
      " [  11   17    1    0    0   26    9  773    9]\n",
      " [   6   25    0    1    0   15    1   19  429]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.72      0.29      0.41       193\n",
      "          2       0.61      0.89      0.73      1612\n",
      "          3       0.53      0.27      0.36       800\n",
      "          4       0.79      0.16      0.27       269\n",
      "          5       0.96      0.95      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.88      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n",
      "Model: \n",
      " LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l1', random_state=42,\n",
      "           refit=True, scoring=None, solver='saga', tol=0.0001, verbose=0)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  56   27    0    0    0   21    2   41   46]\n",
      " [   0 1438  137    6    8    6   10    5    2]\n",
      " [   0  565  217    3    0    1   10    3    1]\n",
      " [   0  168   29   45    4   19    4    0    0]\n",
      " [   0   15    2    0  257    0    0    0    0]\n",
      " [   3   34    1    3    0 1302   23   29   19]\n",
      " [   2   55   20    1    1   30  158   14    3]\n",
      " [  11   16    2    0    0   26    9  773    9]\n",
      " [   6   25    0    1    0   15    1   19  429]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.72      0.29      0.41       193\n",
      "          2       0.61      0.89      0.73      1612\n",
      "          3       0.53      0.27      0.36       800\n",
      "          4       0.76      0.17      0.27       269\n",
      "          5       0.95      0.94      0.94       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.87      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n",
      "Model: \n",
      " LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='multinomial', n_jobs=1, penalty='l1',\n",
      "           random_state=42, refit=True, scoring=None, solver='saga',\n",
      "           tol=0.0001, verbose=0)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  82    7    1    0    0   20    5   33   45]\n",
      " [   2 1407  145   21    8    8   13    4    4]\n",
      " [   0  541  238    5    0    1   11    4    0]\n",
      " [   0  140   25   81    4   13    6    0    0]\n",
      " [   0   18    0    0  255    0    0    0    1]\n",
      " [   6   24    2    3    0 1295   26   36   22]\n",
      " [   3   43   21    3    0   32  166   14    2]\n",
      " [  16    9    2    1    0   25   11  774    8]\n",
      " [  15   11    0    0    0   18    1   17  434]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.66      0.42      0.52       193\n",
      "          2       0.64      0.87      0.74      1612\n",
      "          3       0.55      0.30      0.39       800\n",
      "          4       0.71      0.30      0.42       269\n",
      "          5       0.96      0.93      0.94       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.69      0.58      0.63       284\n",
      "          8       0.88      0.91      0.90       846\n",
      "          9       0.84      0.88      0.86       496\n",
      "\n",
      "avg / total       0.76      0.76      0.75      6188\n",
      "\n",
      "Model: \n",
      " LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=200,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  56   26    0    1    0   21    2   41   46]\n",
      " [   0 1440  138    4    7    6   10    5    2]\n",
      " [   0  566  218    1    0    1   10    3    1]\n",
      " [   0  172   29   41    4   19    4    0    0]\n",
      " [   0   15    2    0  257    0    0    0    0]\n",
      " [   3   34    1    3    1 1301   23   28   20]\n",
      " [   2   55   20    1    0   30  159   14    3]\n",
      " [  11   17    1    0    0   28    9  771    9]\n",
      " [   6   25    0    1    0   15    1   19  429]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.72      0.29      0.41       193\n",
      "          2       0.61      0.89      0.73      1612\n",
      "          3       0.53      0.27      0.36       800\n",
      "          4       0.79      0.15      0.26       269\n",
      "          5       0.96      0.94      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.88      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n",
      "Model: \n",
      " LogisticRegressionCV(Cs=100, class_weight=None, cv=None, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=1000,\n",
      "           multi_class='multinomial', n_jobs=1, penalty='l2',\n",
      "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
      "           tol=0.0001, verbose=0)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  83    8    1    0    0   20    4   34   43]\n",
      " [   2 1405  148   20    6    7   14    5    5]\n",
      " [   0  533  245    6    0    1   11    4    0]\n",
      " [   0  142   25   81    3   12    6    0    0]\n",
      " [   0   14    0    0  260    0    0    0    0]\n",
      " [   6   23    2    4    3 1295   24   35   22]\n",
      " [   3   44   22    3    1   32  162   15    2]\n",
      " [  15    9    2    1    0   24   10  775   10]\n",
      " [  15   11    0    0    1   16    1   20  432]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.43      0.52       193\n",
      "          2       0.64      0.87      0.74      1612\n",
      "          3       0.55      0.31      0.39       800\n",
      "          4       0.70      0.30      0.42       269\n",
      "          5       0.95      0.95      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.70      0.57      0.63       284\n",
      "          8       0.87      0.92      0.89       846\n",
      "          9       0.84      0.87      0.86       496\n",
      "\n",
      "avg / total       0.76      0.77      0.75      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print details and evaluations of models to compare\n",
    "def print_models(models):\n",
    "    for model in models:    \n",
    "        print(\"Model: \\n\",model.get(\"model\"))\n",
    "        print(\"\\nConfustion Matrix: \\n\",model.get(\"confusion_matrix\"))\n",
    "        print(\"\\nClassification Report: \\n\",model.get(\"classification_report\"))\n",
    "        model.update({\"params\":get_imp_params(model.get(\"model\").get_params())})\n",
    "print_models(models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_score  test_score  log_loss_train  log_loss_test  \\\n",
      "0      0.75737     0.75614         0.66104        0.66987   \n",
      "1      0.75708     0.75549         0.66273        0.67156   \n",
      "2      0.76719     0.76471         0.62487        0.63743   \n",
      "3      0.75730     0.75501         0.66162        0.67034   \n",
      "4      0.76786     0.76568         0.62094        0.63614   \n",
      "\n",
      "  params(C,solver,penalty,multi_class)  \n",
      "0             C:10, liblinear, l1, ovr  \n",
      "1                  C:10, saga, l1, ovr  \n",
      "2          C:10, saga, l1, multinomial  \n",
      "3                 C:10, lbfgs, l2, ovr  \n",
      "4        C:100, lbfgs, l2, multinomial  \n"
     ]
    }
   ],
   "source": [
    "def get_model_evaluations(models):\n",
    "    \n",
    "    # compare key metrics like log_loss_test etc.\n",
    "    models_df = pd.DataFrame.from_dict(models)\n",
    "    eval_df = pd.DataFrame(columns=[\"train_score\",\"test_score\",\"log_loss_train\",\\\n",
    "                                    \"log_loss_test\",\"params(C,solver,penalty,multi_class)\"])\n",
    "    eval_df[\"log_loss_train\"] = models_df.log_loss_train\n",
    "    eval_df[\"log_loss_test\"] = models_df.log_loss_test\n",
    "    eval_df[\"train_score\"] = models_df.train_score\n",
    "    eval_df[\"test_score\"] = models_df.test_score\n",
    "    eval_df[\"params(C,solver,penalty,multi_class)\"] = models_df.params\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "\n",
    "print(get_model_evaluations(models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## Dealing with class imbalance\n",
    "\n",
    "<a id='section10'></a>\n",
    "### Over and Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_classes(sm, train_x, train_y, name=\"dummy\"):\n",
    "    \"\"\"\n",
    "    Balance the classes using the given technique\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sm : class balancing model\n",
    "    name: to save the balanced data to the disk\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"original data shape(X,y): \", train_x.shape, train_y.shape)\n",
    "    X_os,y_os = sm.fit_sample(train_x,train_y)\n",
    "    print(\"balanced data shape(X,y): \", X_os.shape, y_os.shape)\n",
    "    \n",
    "    # uncomment if you want to save the balanced data separately\n",
    "    # joblib.dump([X_os,y_os],filename=\"\" + name + \".data\")\n",
    "    \n",
    "    return X_os,y_os\n",
    "\n",
    "\n",
    "def balance_and_fit_lr(sm,train_x, train_y, sm_name=\"dummy\",name=\"dummy\"):\n",
    "    \"\"\"\n",
    "    Fit the Logistic Regression model after balancing the classes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sm : class balancing model\n",
    "    sm_name: to save the balanced data to the disk\n",
    "    name: to save the model to the disk\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple:\n",
    "        Pandas dataframes for X and y having balanced data\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # first time\n",
    "    #X_os, y_os = balance_classes(sm,X_train,y_train,name=sm_name)\n",
    "\n",
    "    # all other times\n",
    "    X_os, y_os = joblib.load(\"../models-repo/\" + sm_name + \".data\")\n",
    "    \n",
    "    # Fit Logistic model with balanced data\n",
    "    lr = LogisticRegression()\n",
    "    fit_evaluate(lr, {\"C\":10},{},X_os,y_os,X_test,y_test,\"neg_log_loss\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:312: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.19.1 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load the saved models\n",
    "balanced_names = ['lr_ros','lr_adasyn']\n",
    "balanced_models = [joblib.load(\"../models-repo/\" + name + \".sav\") for name in balanced_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      " LogisticRegression(C=3, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[ 139    2    2    2    2    3    7   14   22]\n",
      " [  23  912  328  292   14    3   32    3    5]\n",
      " [   2  203  373  175    3    1   40    0    3]\n",
      " [   0   38   28  181    5    7   10    0    0]\n",
      " [   1    3    2    0  268    0    0    0    0]\n",
      " [  57    7    4   22    2 1215   46   22   39]\n",
      " [  19    7   16   17    1    3  214    4    3]\n",
      " [  82    3    4    0    0   12   19  713   13]\n",
      " [  78    5    1    5    1   10    3   11  382]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.72      0.47       193\n",
      "          2       0.77      0.57      0.65      1612\n",
      "          3       0.49      0.47      0.48       800\n",
      "          4       0.26      0.67      0.38       269\n",
      "          5       0.91      0.98      0.94       274\n",
      "          6       0.97      0.86      0.91      1414\n",
      "          7       0.58      0.75      0.65       284\n",
      "          8       0.93      0.84      0.88       846\n",
      "          9       0.82      0.77      0.79       496\n",
      "\n",
      "avg / total       0.77      0.71      0.73      6188\n",
      "\n",
      "Model: \n",
      " LogisticRegression(C=3, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "Confustion Matrix: \n",
      " [[  57   28    1    0    0   22    3   37   45]\n",
      " [   1 1453  122    4    9    5   10    4    4]\n",
      " [   0  574  204    2    0    1   15    3    1]\n",
      " [   0  173   29   42    4   18    3    0    0]\n",
      " [   0   15    1    0  258    0    0    0    0]\n",
      " [   4   34    0    2    2 1301   22   24   25]\n",
      " [   3   53   21    1    0   27  163   13    3]\n",
      " [   8   16    2    0    0   24    9  777   10]\n",
      " [   9   24    0    1    0   16    2   17  427]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.70      0.30      0.41       193\n",
      "          2       0.61      0.90      0.73      1612\n",
      "          3       0.54      0.26      0.35       800\n",
      "          4       0.81      0.16      0.26       269\n",
      "          5       0.95      0.94      0.94       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.72      0.57      0.64       284\n",
      "          8       0.89      0.92      0.90       846\n",
      "          9       0.83      0.86      0.84       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_models(balanced_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_score  test_score  log_loss_train  log_loss_test\n",
      "0      0.73603     0.71057         0.74830        0.79605\n",
      "1      0.75970     0.75663         0.65856        0.67975\n"
     ]
    }
   ],
   "source": [
    "print(get_model_evaluations(balanced_models).drop(\"params(C,solver,penalty,multi_class)\",axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section11'></a>\n",
    "### One vs Rest method after fixing class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow:\n",
    "1. Set the targets for binary classification\n",
    "2. Balance the classes\n",
    "3. Fit model for each class vs all other classes\n",
    "3. For new data, predict the class with highest probability given by each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55690, 93)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1736,\n",
       "         2: 14510,\n",
       "         3: 7204,\n",
       "         4: 2422,\n",
       "         5: 2465,\n",
       "         6: 12721,\n",
       "         7: 2555,\n",
       "         8: 7618,\n",
       "         9: 4459})"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the class imablances\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make labels for one vs rest\n",
    "def make_ovr_label(y, positive_class):\n",
    "    \"\"\"\n",
    "    Convert multi classes into binary classes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : class distribution in target variable\n",
    "    positive_class: postive class, rest of the classes will be called as negative class\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series:\n",
    "        Pandas series for target having binary classes\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"old y distribution: \", Counter(y))\n",
    "    y_new = y.copy()\n",
    "    y_new[y_new!=positive_class] = 0\n",
    "    y_new[y_new==positive_class] = 1\n",
    "    print(\"new y distribution: \", Counter(y_new))\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53954, 1: 1736})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 41180, 1: 14510})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 48486, 1: 7204})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53268, 1: 2422})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53225, 1: 2465})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 42969, 1: 12721})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53135, 1: 2555})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 48072, 1: 7618})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 51231, 1: 4459})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5995, 1: 193})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 4576, 1: 1612})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5388, 1: 800})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5919, 1: 269})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5914, 1: 274})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 4774, 1: 1414})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5904, 1: 284})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5342, 1: 846})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5692, 1: 496})\n"
     ]
    }
   ],
   "source": [
    "# turn all train and test into binary classes\n",
    "y_trains = [make_ovr_label(y_train, positive_class) for positive_class in np.arange(1,10)]\n",
    "y_tests = [make_ovr_label(y_test, positive_class) for positive_class in np.arange(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Balance the binary classes\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_oss = []\n",
    "y_oss =[]\n",
    "for i in np.arange(0,9):\n",
    "    X_os, y_os = rus.fit_sample(X_train,y_trains[i])\n",
    "    X_oss.append(X_os)\n",
    "    y_oss.append(y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrs = [LogisticRegressionCV(cv=5) for i in np.arange(1,10)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "            refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize all the binary classifiers\n",
    "[lrs[i].fit(X_oss[i],y_oss[i]) for i in np.arange(0,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate predictions and probabilities using all the binary classifiers\n",
    "y_preds = []\n",
    "y_pred_probs = []\n",
    "for row in np.arange(0,X_test.shape[0]):\n",
    "    predict_probs = [lrs[i].predict_proba(X_test.iloc[row:row+1,:]).flatten() for i in np.arange(0,9)]\n",
    "    y_preds.append(np.argmax(np.transpose(predict_probs)[1])+1)\n",
    "    # make the predict probabilities sum as 1\n",
    "    y_pred_probs.append(np.transpose(predict_probs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss test:  0.949572140707\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 105    4    0    0    1    5   10   27   41]\n",
      " [  16 1120  247  171   21    2   24    5    6]\n",
      " [   2  330  277  133    4    1   46    3    4]\n",
      " [   0   76   32  140    4   10    7    0    0]\n",
      " [   1    4    3    0  266    0    0    0    0]\n",
      " [  33   12    4   14    1 1259   34   29   28]\n",
      " [  21   14   10   14    2   11  201   10    1]\n",
      " [  38    4    4    1    0   14   15  753   17]\n",
      " [  41    6    0    2    0   12    4   14  417]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.41      0.54      0.47       193\n",
      "          2       0.71      0.69      0.70      1612\n",
      "          3       0.48      0.35      0.40       800\n",
      "          4       0.29      0.52      0.38       269\n",
      "          5       0.89      0.97      0.93       274\n",
      "          6       0.96      0.89      0.92      1414\n",
      "          7       0.59      0.71      0.64       284\n",
      "          8       0.90      0.89      0.89       846\n",
      "          9       0.81      0.84      0.83       496\n",
      "\n",
      "avg / total       0.75      0.73      0.74      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Log loss test: \",log_loss(y_test, y_pred_probs))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, y_preds))\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section12'></a>\n",
    "### Balance using class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3.5643881208397339,\n",
       " 2: 0.42644919212803428,\n",
       " 3: 0.85893639336171268,\n",
       " 4: 2.5548215432608496,\n",
       " 5: 2.5102546765832772,\n",
       " 6: 0.48642227637589636,\n",
       " 7: 2.4218308327897371,\n",
       " 8: 0.81225751874216034,\n",
       " 9: 1.3877052652562856}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "cw\n",
    "\n",
    "cw_pairs = [(i,cw[i-1]) for i in np.arange(1,10)]\n",
    "cw_dict = dict(cw_pairs)\n",
    "cw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weighted Logistic Regression\n",
    "lr_weighted = LogisticRegression(class_weight=cw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0,\n",
       "          class_weight={1: 3.5643881208397339, 2: 0.42644919212803428, 3: 0.85893639336171268, 4: 2.5548215432608496, 5: 2.5102546765832772, 6: 0.48642227637589636, 7: 2.4218308327897371, 8: 0.81225751874216034, 9: 1.3877052652562856},\n",
       "          dual=False, fit_intercept=True, intercept_scaling=1,\n",
       "          max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_weighted.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.75419\n",
      "Test score:  0.75323\n",
      "Log loss train:  0.72042\n",
      "Log loss test:  0.73284\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 121    4    0    1    1    5    7   22   32]\n",
      " [  19 1214  242   86   11    4   24    6    6]\n",
      " [   2  381  311   64    2    1   36    2    1]\n",
      " [   0   93   33  120    5    8    9    0    1]\n",
      " [   1    4    2    0  267    0    0    0    0]\n",
      " [  38   13    3    9    2 1260   34   27   28]\n",
      " [  21   15   16   10    1    7  205    7    2]\n",
      " [  45    5    4    0    0   15   14  754    9]\n",
      " [  49    6    0    2    0   13    4   13  409]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.41      0.63      0.49       193\n",
      "          2       0.70      0.75      0.73      1612\n",
      "          3       0.51      0.39      0.44       800\n",
      "          4       0.41      0.45      0.43       269\n",
      "          5       0.92      0.97      0.95       274\n",
      "          6       0.96      0.89      0.92      1414\n",
      "          7       0.62      0.72      0.66       284\n",
      "          8       0.91      0.89      0.90       846\n",
      "          9       0.84      0.82      0.83       496\n",
      "\n",
      "avg / total       0.76      0.75      0.75      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section13'></a>\n",
    "## Other simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 93), (1000,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we should do cross validation twice, once to find the best param and then to evaluate the model\n",
    "knn = KNeighborsClassifier(weights='distance')\n",
    "selected= np.random.choice(df.index,1000)\n",
    "X = df.drop(\"target\",axis=1).iloc[selected]\n",
    "y = df.target.iloc[selected]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='distance'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [25, 35, 40, 45]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_log_loss', verbose=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"n_neighbors\":[25,35,40,45]}\n",
    "#gcv = RandomizedSearchCV(knn,cv=3,n_jobs=-1,param_distributions=params,scoring='neg_log_loss',verbose=True)\n",
    "gcv = GridSearchCV(knn,cv=3,n_jobs=-1,param_grid=params,scoring='neg_log_loss',verbose=True)\n",
    "gcv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=gcv,X=X,y=y,cv=3,n_jobs=-1,scoring='neg_log_loss',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3968954746308562"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "scores2 = cross_validate(estimator=gcv.best_estimator_,X=X,y=y,cv=3,n_jobs=-1,scoring='neg_log_loss',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3747380524337995"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00733932, 0.00911347, 0.00971063, 0.00501164]),\n",
       " 'mean_score_time': array([0.0470341 , 0.05398655, 0.06497502, 0.04385034]),\n",
       " 'mean_test_score': array([-1.89810491, -1.50121155, -1.37520825, -1.37683696]),\n",
       " 'mean_train_score': array([-8.10462808e-15, -8.10462808e-15, -8.10462808e-15, -8.10462808e-15]),\n",
       " 'param_n_neighbors': masked_array(data=[25, 35, 40, 45],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 25},\n",
       "  {'n_neighbors': 35},\n",
       "  {'n_neighbors': 40},\n",
       "  {'n_neighbors': 45}],\n",
       " 'rank_test_score': array([4, 3, 1, 2], dtype=int32),\n",
       " 'split0_test_score': array([-2.04272732, -1.65457106, -1.49839293, -1.53485505]),\n",
       " 'split0_train_score': array([-8.10462808e-15, -8.10462808e-15, -8.10462808e-15, -8.10462808e-15]),\n",
       " 'split1_test_score': array([-1.67279368, -1.46051687, -1.30070121, -1.33071135]),\n",
       " 'split1_train_score': array([-8.10462808e-15, -8.10462808e-15, -8.10462808e-15, -8.10462808e-15]),\n",
       " 'split2_test_score': array([-1.97797052, -1.38647599, -1.32512002, -1.26283619]),\n",
       " 'split2_train_score': array([-8.10462808e-15, -8.10462808e-15, -8.10462808e-15, -8.10462808e-15]),\n",
       " 'std_fit_time': array([0.00029   , 0.00036043, 0.00089232, 0.00015525]),\n",
       " 'std_score_time': array([0.00328038, 0.00064001, 0.01002253, 0.00342311]),\n",
       " 'std_test_score': array([0.16138073, 0.11318686, 0.08819085, 0.11575854]),\n",
       " 'std_train_score': array([9.10898112e-31, 9.10898112e-31, 9.10898112e-31, 9.10898112e-31])}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5378404185091514"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.cv_results_[\"mean_test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=40, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(weights='distance',n_neighbors=40,n_jobs=-1)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7910542653441273"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_pred=knn.predict_proba(X_test),y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(estimator=knn,X=df.drop(\"target\",axis=1),y=df.target,cv=3,n_jobs=-1,scoring='neg_log_loss',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([3.79166007, 3.17578793, 5.36287308]),\n",
       " 'score_time': array([70.39256716, 73.21098399, 69.21527386]),\n",
       " 'test_score': array([-0.87746544, -0.85461572, -0.82274036]),\n",
       " 'train_score': array([-8.10462808e-15, -8.10462808e-15, -8.10462808e-15])}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_indices = [sample(list(df[df.target==target].index),1929) for target in np.arange(1,10)]\n",
    "\n",
    "balanced_df = pd.DataFrame()\n",
    "# balanced_df = balanced_df.join(df.iloc[target_indices[0]])\n",
    "\n",
    "for target in np.arange(0,9):\n",
    "     balanced_df = balanced_df.append(df.iloc[target_indices[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>1.070815</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>0.531842</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>0.402093</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>4.394992</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>0.782741</td>\n",
       "      <td>1.036270</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>3.448709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>1.070815</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>0.631001</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>0.782741</td>\n",
       "      <td>0.592959</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>0.384940</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>0.631001</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>1.036270</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>0.059129</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772299</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "1046 -0.253508 -0.210106 -0.307165 -0.279443 -0.161867 -0.119331 -0.188045   \n",
       "1391  0.402093 -0.210106 -0.307165 -0.279443  4.394992 -0.119331  0.782741   \n",
       "556  -0.253508 -0.210106 -0.307165 -0.279443 -0.161867 -0.119331  0.782741   \n",
       "1160 -0.253508 -0.210106 -0.307165 -0.279443 -0.161867 -0.119331 -0.188045   \n",
       "1748 -0.253508 -0.210106 -0.307165 -0.279443 -0.161867 -0.119331 -0.188045   \n",
       "\n",
       "             7         8         9   ...          84        85        86  \\\n",
       "1046 -0.293664 -0.291038  0.679472   ...    0.246100  1.070815 -0.249802   \n",
       "1391  1.036270 -0.291038  3.448709   ...   -0.280099  1.070815 -0.249802   \n",
       "556   0.592959 -0.291038 -0.243606   ...   -0.280099 -0.047949  0.384940   \n",
       "1160  1.036270 -0.291038 -0.243606   ...   -0.280099 -0.420870 -0.249802   \n",
       "1748 -0.293664 -0.291038 -0.243606   ...    0.772299 -0.420870 -0.249802   \n",
       "\n",
       "            87        88        89        90        91        92  target  \n",
       "1046  0.531842 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "1391 -0.413584 -0.299712 -0.176699 -0.129516  0.631001 -0.104963       1  \n",
       "556  -0.413584 -0.299712  0.040798 -0.129516  0.631001 -0.104963       1  \n",
       "1160  0.059129 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "1748 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(balanced_df.drop(\"target\",axis=1),balanced_df.target,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12152, 93), (5209, 93), (12152,), (5209,))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=40, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(weights='distance',n_neighbors=40,n_jobs=-1)\n",
    "knn2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803243668926108"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_pred=knn2.predict_proba(X_test),y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='section14'></a>\n",
    "## Please refer other notebooks for Bagging, Boosting and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
