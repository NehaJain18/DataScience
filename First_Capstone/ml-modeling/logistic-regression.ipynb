{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to follow\n",
    "\n",
    "1. Get train test split\n",
    "1. Fit default model. Also fit using cross validation\n",
    "2. Evaluate the model\n",
    "3. Change parameters and hyperparameters\n",
    "4. Evaluate all models\n",
    "5. Compare all models\n",
    "6. Find out feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imoprt libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit,cross_validate\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss,classification_report,confusion_matrix, roc_curve, roc_auc_score,accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# over sampling for imbalanced classes\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE, ADASYN\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, EditedNearestNeighbours, RepeatedEditedNearestNeighbours,InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# calculate weights for imbablanced classes\n",
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.402093</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>0.149647</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>0.149647</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.402093</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>0.079240</td>\n",
       "      <td>13.508710</td>\n",
       "      <td>4.524667</td>\n",
       "      <td>4.665884</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280099</td>\n",
       "      <td>-0.047949</td>\n",
       "      <td>1.019683</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>-0.176699</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.210106</td>\n",
       "      <td>-0.307165</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>-0.161867</td>\n",
       "      <td>-0.119331</td>\n",
       "      <td>-0.188045</td>\n",
       "      <td>-0.293664</td>\n",
       "      <td>-0.291038</td>\n",
       "      <td>-0.243606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>-0.420870</td>\n",
       "      <td>-0.249802</td>\n",
       "      <td>-0.413584</td>\n",
       "      <td>-0.299712</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>-0.129516</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-0.104963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3          4         5         6  \\\n",
       "0  0.402093 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "1 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "2 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "3  0.402093 -0.210106 -0.307165  0.079240  13.508710  4.524667  4.665884   \n",
       "4 -0.253508 -0.210106 -0.307165 -0.279443  -0.161867 -0.119331 -0.188045   \n",
       "\n",
       "          7         8         9   ...          84        85        86  \\\n",
       "0 -0.293664 -0.291038 -0.243606   ...    0.246100 -0.420870 -0.249802   \n",
       "1  0.149647 -0.291038 -0.243606   ...   -0.280099 -0.420870 -0.249802   \n",
       "2  0.149647 -0.291038 -0.243606   ...   -0.280099 -0.420870 -0.249802   \n",
       "3 -0.293664 -0.291038  0.679472   ...   -0.280099 -0.047949  1.019683   \n",
       "4 -0.293664 -0.291038 -0.243606   ...    0.246100 -0.420870 -0.249802   \n",
       "\n",
       "         87        88        89        90        91        92  target  \n",
       "0 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "1 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "2 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "3 -0.413584 -0.299712 -0.176699 -0.129516 -0.386938 -0.104963       1  \n",
       "4 -0.413584 -0.299712  0.040798 -0.129516 -0.386938 -0.104963       1  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should I use normalized data for all models?\n",
    "# using pca components instead of normal features is a good idea? I don't think so.\n",
    "# for logistic regression it is better to use nomralized data\n",
    "\n",
    "df = pd.read_csv(\"../data/train_norm.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.1,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# here the classes are imbalanced so we should use stratified split \n",
    "# the folds are made by preserving the percentage of samples for each class\n",
    "# note that the imbalance will still be their when we train the model using this split\n",
    "\n",
    "# since we have big amount of data our test set can be just 1% of all data\n",
    "sss = StratifiedShuffleSplit(n_splits=1,test_size=0.1, random_state=42)\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [57972 30244  9427 ..., 60232 28576 27516] TEST: [59081 21681 51999 ...,  1777   269 53901]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(\"target\",axis=1)\n",
    "y = df.target\n",
    "\n",
    "train_index = []\n",
    "test_index = []\n",
    "\n",
    "for tr, tes in sss.split(X,y):\n",
    "    print(\"TRAIN:\", tr, \"TEST:\", tes)\n",
    "    train_index = tr\n",
    "    test_index = tes\n",
    "\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of data sets\n",
      "X_train:  (55690, 93) y_train:  (55690,)\n",
      "X_train:  (6188, 93) y_test:  (6188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of data sets\")\n",
    "print(\"X_train: \", X_train.shape, \"y_train: \", y_train.shape)\n",
    "print(\"X_train: \", X_test.shape,\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(m):\n",
    "    print(\"Train score: \",m.score(X_train,y_train).round(5))\n",
    "    print(\"Test score: \",m.score(X_test,y_test).round(5))\n",
    "    print(\"Log loss train: \",log_loss(y_train, m.predict_proba(X_train)).round(5))\n",
    "    print(\"Log loss test: \",log_loss(y_test, m.predict_proba(X_test)).round(5))\n",
    "    print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, m.predict(X_test),labels=m.classes_))\n",
    "    print(\"\\nClassification Report: \\n\", classification_report(y_test, m.predict(X_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(m,y_train,y_test):\n",
    "    print(\"Train score: \",m.score(X_train,y_train).round(5))\n",
    "    print(\"Test score: \",m.score(X_test,y_test).round(5))\n",
    "    print(\"Log loss train: \",log_loss(y_train, m.predict_proba(X_train)).round(5))\n",
    "    print(\"Log loss test: \",log_loss(y_test, m.predict_proba(X_test)).round(5))\n",
    "    print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, m.predict(X_test),labels=m.classes_))\n",
    "    print(\"\\nClassification Report: \\n\", classification_report(y_test, m.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_imp(m):\n",
    "    coef_df = pd.DataFrame(m.coef_)\n",
    "    coef_df = np.absolute(np.round(coef_df,5))\n",
    "\n",
    "    imp_features = [0]*9\n",
    "    for row in np.arange(0,9,1):\n",
    "        # top coefficients for each class\n",
    "        imp_features[row] = sorted(enumerate(coef_df.iloc[row]), \\\n",
    "                                   key=lambda x:x[1],reverse=True)[0:5]\n",
    "\n",
    "    print(\"Most important features: \\n\",np.unique(np.transpose(imp_features)[0])+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.75716\n",
      "Test score:  0.75533\n",
      "Log loss train:  0.66281\n",
      "Log loss test:  0.67151\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  56   27    0    0    0   21    2   41   46]\n",
      " [   0 1438  138    5    8    6   10    5    2]\n",
      " [   0  565  217    3    0    1   10    3    1]\n",
      " [   0  168   29   45    4   19    4    0    0]\n",
      " [   0   15    2    0  257    0    0    0    0]\n",
      " [   3   34    1    3    0 1301   23   29   20]\n",
      " [   2   55   20    1    0   30  159   14    3]\n",
      " [  10   16    2    0    0   27   10  772    9]\n",
      " [   5   26    0    1    0   15    1   19  429]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.74      0.29      0.42       193\n",
      "          2       0.61      0.89      0.73      1612\n",
      "          3       0.53      0.27      0.36       800\n",
      "          4       0.78      0.17      0.28       269\n",
      "          5       0.96      0.94      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.87      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: \n",
      " [  9.  11.  14.  15.  26.  27.  34.  39.  40.  42.  43.  45.  47.  58.  60.\n",
      "  69.  73.  75.  76.  78.  83.  84.  86.  90.]\n"
     ]
    }
   ],
   "source": [
    "feature_imp(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using LogisticRegressionCV\n",
    "lrcv = LogisticRegressionCV(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.75725\n",
      "Test score:  0.75727\n",
      "Log loss:  0.6707\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  57   25    0    0    0   21    3   41   46]\n",
      " [   0 1442  137    4    6    6   10    5    2]\n",
      " [   0  561  221    3    0    1   10    3    1]\n",
      " [   0  168   30   44    4   19    4    0    0]\n",
      " [   0   13    2    0  259    0    0    0    0]\n",
      " [   3   35    1    3    1 1302   23   27   19]\n",
      " [   2   53   20    1    1   30  160   14    3]\n",
      " [  11   17    1    0    0   26    9  773    9]\n",
      " [   5   27    0    1    0   15    1   19  428]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.73      0.30      0.42       193\n",
      "          2       0.62      0.89      0.73      1612\n",
      "          3       0.54      0.28      0.36       800\n",
      "          4       0.79      0.16      0.27       269\n",
      "          5       0.96      0.95      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.88      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lrcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: \n",
      " [  9.  11.  14.  15.  26.  27.  36.  39.  40.  42.  43.  45.  47.  58.  59.\n",
      "  60.  68.  69.  73.  75.  76.  83.  84.  86.  90.]\n"
     ]
    }
   ],
   "source": [
    "feature_imp(lrcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross validation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 15.76905894,  15.40101194,  14.45040584]),\n",
       " 'score_time': array([ 0.01995587,  0.0199542 ,  0.01813412]),\n",
       " 'test_score': array([-0.68030046, -0.67570131, -0.6716733 ]),\n",
       " 'train_score': array([-0.65810914, -0.66169568, -0.66303738])}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(estimator=lr,cv=3,X=X_train,y=y_train,scoring=('neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive random over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = RandomOverSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_os, y_os = rs.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130590, 93), (130590,))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_os.shape, y_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X=X_os,y=y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.71501\n",
      "Test score:  0.71025\n",
      "Log loss train:  0.78172\n",
      "Log loss test:  0.79707\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 140    2    2    2    2    3    7   14   21]\n",
      " [  23  909  329  291   17    3   32    3    5]\n",
      " [   2  205  371  175    2    1   41    0    3]\n",
      " [   0   38   28  181    5    7   10    0    0]\n",
      " [   1    3    2    0  268    0    0    0    0]\n",
      " [  58    7    4   22    2 1215   46   21   39]\n",
      " [  19    7   16   17    0    3  215    4    3]\n",
      " [  81    3    4    0    0   12   19  714   13]\n",
      " [  79    5    1    5    0   10    3   11  382]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.73      0.47       193\n",
      "          2       0.77      0.56      0.65      1612\n",
      "          3       0.49      0.46      0.48       800\n",
      "          4       0.26      0.67      0.38       269\n",
      "          5       0.91      0.98      0.94       274\n",
      "          6       0.97      0.86      0.91      1414\n",
      "          7       0.58      0.76      0.65       284\n",
      "          8       0.93      0.84      0.89       846\n",
      "          9       0.82      0.77      0.79       496\n",
      "\n",
      "avg / total       0.77      0.71      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features: \n",
      " [ 11.  14.  15.  19.  26.  34.  39.  43.  45.  47.  58.  59.  60.  69.  73.\n",
      "  75.  76.  78.  83.  84.  86.  90.  91.  92.]\n"
     ]
    }
   ],
   "source": [
    "feature_imp(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above naive random over sampling did worse results then our original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE, ADASYN oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_os, y_os = smote.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130590, 93), (130590,))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_os.shape, y_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X=X_os,y=y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.71354\n",
      "Test score:  0.70814\n",
      "Log loss train:  0.76569\n",
      "Log loss test:  0.78492\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 137    3    1    2    1    3    8   17   21]\n",
      " [  27  866  350  304   13    4   40    3    5]\n",
      " [   2  185  382  184    2    1   42    0    2]\n",
      " [   0   37   29  181    5    7   10    0    0]\n",
      " [   1    3    2    0  268    0    0    0    0]\n",
      " [  55    7    3   21    2 1224   45   21   36]\n",
      " [  19    6   14   15    0    4  217    5    4]\n",
      " [  68    2    4    0    0   12   23  724   13]\n",
      " [  77    4    1    5    0   12    3   11  383]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.71      0.47       193\n",
      "          2       0.78      0.54      0.64      1612\n",
      "          3       0.49      0.48      0.48       800\n",
      "          4       0.25      0.67      0.37       269\n",
      "          5       0.92      0.98      0.95       274\n",
      "          6       0.97      0.87      0.91      1414\n",
      "          7       0.56      0.76      0.65       284\n",
      "          8       0.93      0.86      0.89       846\n",
      "          9       0.83      0.77      0.80       496\n",
      "\n",
      "avg / total       0.77      0.71      0.72      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adasyn = ADASYN(n_neighbors=5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_os, y_os = adasyn.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131124, 93), (131124,))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_os.shape, y_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X=X_os,y=y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.69154\n",
      "Test score:  0.68714\n",
      "Log loss train:  0.81283\n",
      "Log loss test:  0.8348\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 122    2    1    1    2    2   12   18   33]\n",
      " [  25  778  334  327   70    1   53    9   15]\n",
      " [   2  159  351  200   20    1   55    8    4]\n",
      " [   0   24   27  187    9    6   13    0    3]\n",
      " [   0    1    2    0  271    0    0    0    0]\n",
      " [  44    3    1   11   11 1199   53   36   56]\n",
      " [  14    4   10   16    3    4  223    6    4]\n",
      " [  54    4    2    0    3    8   24  732   19]\n",
      " [  69    1    2    3    7    7    6   12  389]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.37      0.63      0.47       193\n",
      "          2       0.80      0.48      0.60      1612\n",
      "          3       0.48      0.44      0.46       800\n",
      "          4       0.25      0.70      0.37       269\n",
      "          5       0.68      0.99      0.81       274\n",
      "          6       0.98      0.85      0.91      1414\n",
      "          7       0.51      0.79      0.62       284\n",
      "          8       0.89      0.87      0.88       846\n",
      "          9       0.74      0.78      0.76       496\n",
      "\n",
      "avg / total       0.75      0.69      0.70      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype generation under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = ClusterCentroids(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_os, y_os = cc.fit_sample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15624, 93), (15624,))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_os.shape, y_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X=X_os,y=y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.59224\n",
      "Test score:  0.58759\n",
      "Log loss train:  0.97031\n",
      "Log loss test:  0.99595\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 149    2    2    3    2    3    6    9   17]\n",
      " [  72  453  280  744   36    1   20    2    4]\n",
      " [  16   99  312  337    7    1   27    0    1]\n",
      " [   6   14   17  218    6    3    5    0    0]\n",
      " [   3    1    0    1  269    0    0    0    0]\n",
      " [ 136    4    5   50    4 1131   37   15   32]\n",
      " [  34    3   14   33    2    1  191    3    3]\n",
      " [ 187    5    6    6    4   11   21  592   14]\n",
      " [ 155    1    0    7    0    5    1    6  321]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.20      0.77      0.31       193\n",
      "          2       0.78      0.28      0.41      1612\n",
      "          3       0.49      0.39      0.43       800\n",
      "          4       0.16      0.81      0.26       269\n",
      "          5       0.82      0.98      0.89       274\n",
      "          6       0.98      0.80      0.88      1414\n",
      "          7       0.62      0.67      0.65       284\n",
      "          8       0.94      0.70      0.80       846\n",
      "          9       0.82      0.65      0.72       496\n",
      "\n",
      "avg / total       0.76      0.59      0.62      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15624, 93) (15624,)\n",
      "Train score:  0.71424\n",
      "Test score:  0.71202\n",
      "Log loss train:  0.79894\n",
      "Log loss test:  0.81376\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 137    0    2    2    2    3    8   17   22]\n",
      " [  22  927  320  275   20    2   35    4    7]\n",
      " [   3  207  368  176    6    1   35    2    2]\n",
      " [   0   38   33  177    6    7    8    0    0]\n",
      " [   2    3    2    0  267    0    0    0    0]\n",
      " [  53    6    5   24    1 1216   49   26   34]\n",
      " [  20    6   20   14    1    5  212    4    2]\n",
      " [  76    3    5    0    0   12   19  717   14]\n",
      " [  79    4    1    4    0    9    5    9  385]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.71      0.47       193\n",
      "          2       0.78      0.58      0.66      1612\n",
      "          3       0.49      0.46      0.47       800\n",
      "          4       0.26      0.66      0.38       269\n",
      "          5       0.88      0.97      0.93       274\n",
      "          6       0.97      0.86      0.91      1414\n",
      "          7       0.57      0.75      0.65       284\n",
      "          8       0.92      0.85      0.88       846\n",
      "          9       0.83      0.78      0.80       496\n",
      "\n",
      "avg / total       0.77      0.71      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_os, y_os = rus.fit_sample(X_train,y_train)\n",
    "print(X_os.shape, y_os.shape)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X=X_os,y=y_os)\n",
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near miss under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_evaluate(sampling, X_train, y_train):\n",
    "    X_os, y_os = sampling.fit_sample(X_train,y_train)\n",
    "    print(X_os.shape, y_os.shape)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X=X_os,y=y_os)\n",
    "    evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm1 = NearMiss(random_state=0, version=1,n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15624, 93) (15624,)\n",
      "Train score:  0.53974\n",
      "Test score:  0.54945\n",
      "Log loss train:  2.37182\n",
      "Log loss test:  2.35544\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 111    6    0    4    1    8    9   17   37]\n",
      " [  23  337  226  739   76    9  186   10    6]\n",
      " [   2   59  161  320   41    2  202   12    1]\n",
      " [   0   16   31  191    4    4   22    0    1]\n",
      " [   0    4    0    4  265    0    0    0    1]\n",
      " [  82    5    0   47    2 1137   97   25   19]\n",
      " [  11   10    7    8    1    6  235    4    2]\n",
      " [  90    5    1    5    0   16   63  651   15]\n",
      " [ 121    9    1   21    0   10    7   15  312]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.25      0.58      0.35       193\n",
      "          2       0.75      0.21      0.33      1612\n",
      "          3       0.38      0.20      0.26       800\n",
      "          4       0.14      0.71      0.24       269\n",
      "          5       0.68      0.97      0.80       274\n",
      "          6       0.95      0.80      0.87      1414\n",
      "          7       0.29      0.83      0.43       284\n",
      "          8       0.89      0.77      0.82       846\n",
      "          9       0.79      0.63      0.70       496\n",
      "\n",
      "avg / total       0.70      0.55      0.56      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_os, y_os = nm1.fit_sample(X_train,y_train)\n",
    "print(X_os.shape, y_os.shape)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X=X_os,y=y_os)\n",
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm2 = NearMiss(random_state=42, version=2,n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15624, 93) (15624,)\n",
      "Train score:  0.57298\n",
      "Test score:  0.57143\n",
      "Log loss train:  1.01909\n",
      "Log loss test:  1.03002\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 112    1    2    6    1    4   10   13   44]\n",
      " [  10  300  123 1073   32    5   43    3   23]\n",
      " [   1  115  161  477    3    0   32    3    8]\n",
      " [   0   12    4  238    5    4    6    0    0]\n",
      " [   0    1    2    2  267    0    0    1    1]\n",
      " [  31    3    3   66    4 1193   47   19   48]\n",
      " [  11    3    6   36    1    6  212    5    4]\n",
      " [  53    7    6    4    0   29   23  683   41]\n",
      " [  74    1    3   19    0   11    7   11  370]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.38      0.58      0.46       193\n",
      "          2       0.68      0.19      0.29      1612\n",
      "          3       0.52      0.20      0.29       800\n",
      "          4       0.12      0.88      0.22       269\n",
      "          5       0.85      0.97      0.91       274\n",
      "          6       0.95      0.84      0.89      1414\n",
      "          7       0.56      0.75      0.64       284\n",
      "          8       0.93      0.81      0.86       846\n",
      "          9       0.69      0.75      0.71       496\n",
      "\n",
      "avg / total       0.72      0.57      0.59      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate(nm2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nm3 = NearMiss(random_state=42, version=3,n_neighbors_ver3=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/imblearn/under_sampling/prototype_selection/nearmiss.py:211: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn('The number of the samples to be selected is larger'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11781, 93) (11781,)\n",
      "Train score:  0.69962\n",
      "Test score:  0.70023\n",
      "Log loss train:  0.88521\n",
      "Log loss test:  0.89183\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  97    7    4    1    2   15    2   26   39]\n",
      " [  10  888  395  268   20    8   15    6    2]\n",
      " [   1  300  373   94    3    1   26    2    0]\n",
      " [   1   55   60  133    5    7    7    0    1]\n",
      " [   0    2    3    0  266    0    0    2    1]\n",
      " [   8   15    4   10   15 1280   35   28   19]\n",
      " [   7   19   30    9    0   15  189   13    2]\n",
      " [  21    8   11    1    2   59   18  716   10]\n",
      " [  46   10    3    4    0   18    1   23  391]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.51      0.50      0.51       193\n",
      "          2       0.68      0.55      0.61      1612\n",
      "          3       0.42      0.47      0.44       800\n",
      "          4       0.26      0.49      0.34       269\n",
      "          5       0.85      0.97      0.91       274\n",
      "          6       0.91      0.91      0.91      1414\n",
      "          7       0.65      0.67      0.66       284\n",
      "          8       0.88      0.85      0.86       846\n",
      "          9       0.84      0.79      0.81       496\n",
      "\n",
      "avg / total       0.72      0.70      0.71      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate(nm3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enn = EditedNearestNeighbours(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32804, 93) (32804,)\n",
      "Train score:  0.75139\n",
      "Test score:  0.7521\n",
      "Log loss train:  0.81867\n",
      "Log loss test:  0.82391\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 118   16    0    1    1   12    3   21   21]\n",
      " [  10 1454  112    5    9    9    9    3    1]\n",
      " [   3  570  199    2    0    1   21    2    2]\n",
      " [   0  181   35   25    4   18    5    1    0]\n",
      " [   0   14    1    0  259    0    0    0    0]\n",
      " [  26   32    0    2    0 1298   20   20   16]\n",
      " [  31   37   21    0    1   19  160   13    2]\n",
      " [  40   11    1    1    0   22    8  759    4]\n",
      " [  64   17    1    0    0   14    3   15  382]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.40      0.61      0.49       193\n",
      "          2       0.62      0.90      0.74      1612\n",
      "          3       0.54      0.25      0.34       800\n",
      "          4       0.69      0.09      0.16       269\n",
      "          5       0.95      0.95      0.95       274\n",
      "          6       0.93      0.92      0.92      1414\n",
      "          7       0.70      0.56      0.62       284\n",
      "          8       0.91      0.90      0.90       846\n",
      "          9       0.89      0.77      0.83       496\n",
      "\n",
      "avg / total       0.76      0.75      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate(enn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iht = InstanceHardnessThreshold(random_state=42, estimator=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15624, 93) (15624,)\n",
      "Train score:  0.52932\n",
      "Test score:  0.52666\n",
      "Log loss train:  1.87048\n",
      "Log loss test:  1.91493\n",
      "\n",
      "Confusion Matrix: \n",
      " [[166   1   0   3   0   1  10   3   9]\n",
      " [125 630 269 541  14   2  29   0   2]\n",
      " [ 25 125 319 294   4   0  32   0   1]\n",
      " [  9  17  16 212   4   2   9   0   0]\n",
      " [ 14   3   5   2 249   0   0   0   1]\n",
      " [348  14   7 129   2 766 126   9  13]\n",
      " [ 37   4  12  22   1   0 205   1   2]\n",
      " [336   6  17  13   0   6  27 433   8]\n",
      " [197   5   2   7   0   0   4   2 279]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.13      0.86      0.23       193\n",
      "          2       0.78      0.39      0.52      1612\n",
      "          3       0.49      0.40      0.44       800\n",
      "          4       0.17      0.79      0.28       269\n",
      "          5       0.91      0.91      0.91       274\n",
      "          6       0.99      0.54      0.70      1414\n",
      "          7       0.46      0.72      0.56       284\n",
      "          8       0.97      0.51      0.67       846\n",
      "          9       0.89      0.56      0.69       496\n",
      "\n",
      "avg / total       0.77      0.53      0.58      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate(iht, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of over and under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smote_enn = SMOTEENN(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110862, 93) (110862,)\n",
      "Train score:  0.64387\n",
      "Test score:  0.63849\n",
      "Log loss train:  1.0515\n",
      "Log loss test:  1.07592\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 138    1    2    3    1    2   11   14   21]\n",
      " [  32  444  427  616   21    2   58    3    9]\n",
      " [   3   48  378  305    4    1   58    1    2]\n",
      " [   1    6   22  217    5    6   12    0    0]\n",
      " [   1    0    4    1  268    0    0    0    0]\n",
      " [  72    2    5   25    2 1190   54   21   43]\n",
      " [  18    1    8   22    1    3  225    2    4]\n",
      " [  78    2    5    1    0    9   22  713   16]\n",
      " [  85    1    2    6    0    7    6   11  378]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.32      0.72      0.44       193\n",
      "          2       0.88      0.28      0.42      1612\n",
      "          3       0.44      0.47      0.46       800\n",
      "          4       0.18      0.81      0.30       269\n",
      "          5       0.89      0.98      0.93       274\n",
      "          6       0.98      0.84      0.90      1414\n",
      "          7       0.50      0.79      0.62       284\n",
      "          8       0.93      0.84      0.89       846\n",
      "          9       0.80      0.76      0.78       496\n",
      "\n",
      "avg / total       0.78      0.64      0.65      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_evaluate(smote_enn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3.5643881208397339,\n",
       " 2: 0.42644919212803428,\n",
       " 3: 0.85893639336171268,\n",
       " 4: 2.5548215432608496,\n",
       " 5: 2.5102546765832772,\n",
       " 6: 0.48642227637589636,\n",
       " 7: 2.4218308327897371,\n",
       " 8: 0.81225751874216034,\n",
       " 9: 1.3877052652562856}"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "cw\n",
    "\n",
    "cw_pairs = [(i,cw[i-1]) for i in np.arange(1,10)]\n",
    "cw_dict = dict(cw_pairs)\n",
    "cw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_weighted = LogisticRegression(class_weight=cw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0,\n",
       "          class_weight={1: 3.5643881208397339, 2: 0.42644919212803428, 3: 0.85893639336171268, 4: 2.5548215432608496, 5: 2.5102546765832772, 6: 0.48642227637589636, 7: 2.4218308327897371, 8: 0.81225751874216034, 9: 1.3877052652562856},\n",
       "          dual=False, fit_intercept=True, intercept_scaling=1,\n",
       "          max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_weighted.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.75419\n",
      "Test score:  0.75323\n",
      "Log loss train:  0.72042\n",
      "Log loss test:  0.73284\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 121    4    0    1    1    5    7   22   32]\n",
      " [  19 1214  242   86   11    4   24    6    6]\n",
      " [   2  381  311   64    2    1   36    2    1]\n",
      " [   0   93   33  120    5    8    9    0    1]\n",
      " [   1    4    2    0  267    0    0    0    0]\n",
      " [  38   13    3    9    2 1260   34   27   28]\n",
      " [  21   15   16   10    1    7  205    7    2]\n",
      " [  45    5    4    0    0   15   14  754    9]\n",
      " [  49    6    0    2    0   13    4   13  409]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.41      0.63      0.49       193\n",
      "          2       0.70      0.75      0.73      1612\n",
      "          3       0.51      0.39      0.44       800\n",
      "          4       0.41      0.45      0.43       269\n",
      "          5       0.92      0.97      0.95       274\n",
      "          6       0.96      0.89      0.92      1414\n",
      "          7       0.62      0.72      0.66       284\n",
      "          8       0.91      0.89      0.90       846\n",
      "          9       0.84      0.82      0.83       496\n",
      "\n",
      "avg / total       0.76      0.75      0.75      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try one vs all approach\n",
    "\n",
    "Steps to follow:\n",
    "1. Fit model for each class vs all other classes\n",
    "2. Use the balanced classes for the above\n",
    "3. For new data, predict the class with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55690, 93)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1736,\n",
       "         2: 14510,\n",
       "         3: 7204,\n",
       "         4: 2422,\n",
       "         5: 2465,\n",
       "         6: 12721,\n",
       "         7: 2555,\n",
       "         8: 7618,\n",
       "         9: 4459})"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make labels for one vs rest\n",
    "def make_ovr_label(y, one_class):\n",
    "    print(\"old y distribution: \", Counter(y))\n",
    "    y_new = y.copy()\n",
    "    y_new[y_new!=one_class] = 0\n",
    "    y_new[y_new==one_class] = 1\n",
    "    print(\"new y distribution: \", Counter(y_new))\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53954, 1: 1736})\n"
     ]
    }
   ],
   "source": [
    "y_train_1 = make_ovr_label(y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5995, 1: 193})\n"
     ]
    }
   ],
   "source": [
    "y_test_1 = make_ovr_label(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_1 = LogisticRegression()\n",
    "lr_1.fit(X_train,y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.97184\n",
      "Test score:  0.9722\n",
      "Log loss train:  0.08113\n",
      "Log loss test:  0.08773\n",
      "\n",
      "Confusion Matrix: \n",
      " [[5982   13]\n",
      " [ 159   34]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      5995\n",
      "          1       0.72      0.18      0.28       193\n",
      "\n",
      "avg / total       0.97      0.97      0.96      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lr_1,y_train_1,y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SMOTE ENN sampling for class imbalance\n",
    "def fit_evaluate(sampling, X_train, y_train, X_test, y_test):\n",
    "    X_os, y_os = sampling.fit_sample(X_train,y_train)\n",
    "    print(X_os.shape, y_os.shape)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X=X_os,y=y_os)\n",
    "    evaluate_model(lr,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103242, 93) (103242,)\n",
      "Train score:  0.85015\n",
      "Test score:  0.84712\n",
      "Log loss train:  0.37417\n",
      "Log loss test:  0.38388\n",
      "\n",
      "Confusion Matrix: \n",
      " [[5073  922]\n",
      " [  24  169]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.91      5995\n",
      "          1       0.15      0.88      0.26       193\n",
      "\n",
      "avg / total       0.97      0.85      0.89      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SMOTE ENN sampling for class imbalance\n",
    "fit_evaluate(smote_enn, X_train, y_train_1, X_test, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53954, 1: 1736})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 41180, 1: 14510})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 48486, 1: 7204})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53268, 1: 2422})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53225, 1: 2465})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 42969, 1: 12721})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 53135, 1: 2555})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 48072, 1: 7618})\n",
      "old y distribution:  Counter({2: 14510, 6: 12721, 8: 7618, 3: 7204, 9: 4459, 7: 2555, 5: 2465, 4: 2422, 1: 1736})\n",
      "new y distribution:  Counter({0: 51231, 1: 4459})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5995, 1: 193})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 4576, 1: 1612})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5388, 1: 800})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5919, 1: 269})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5914, 1: 274})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 4774, 1: 1414})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5904, 1: 284})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5342, 1: 846})\n",
      "old y distribution:  Counter({2: 1612, 6: 1414, 8: 846, 3: 800, 9: 496, 7: 284, 5: 274, 4: 269, 1: 193})\n",
      "new y distribution:  Counter({0: 5692, 1: 496})\n"
     ]
    }
   ],
   "source": [
    "y_trains = [make_ovr_label(y_train, i) for i in np.arange(1,10)]\n",
    "y_tests = [make_ovr_label(y_test, i) for i in np.arange(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [LogisticRegression() for i in np.arange(1,10)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lrs[i].fit(X_train,y_trains[i]) for i in np.arange(0,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probs = [lrs[i].predict_proba(X_test.iloc[0:1,:]).flatten() for i in np.arange(0,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.80384653,  0.19615347]),\n",
       " array([ 0.90601923,  0.09398077]),\n",
       " array([ 0.7894577,  0.2105423]),\n",
       " array([ 0.99716568,  0.00283432]),\n",
       " array([  9.99999986e-01,   1.43134517e-08]),\n",
       " array([ 0.83294029,  0.16705971]),\n",
       " array([ 0.90321867,  0.09678133]),\n",
       " array([ 0.78114804,  0.21885196]),\n",
       " array([ 0.65925352,  0.34074648])]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "y_pred_probs = []\n",
    "for row in np.arange(0,X_test.shape[0]):\n",
    "    predict_probs = [lrs[i].predict_proba(X_test.iloc[row:row+1,:]).flatten() for i in np.arange(0,9)]\n",
    "    y_preds.append(np.argmax(np.transpose(predict_probs)[1])+1)\n",
    "    y_pred_probs.append(np.transpose(predict_probs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.96153474e-01,   9.39807717e-02,   2.10542301e-01,\n",
       "         2.83432245e-03,   1.43134517e-08,   1.67059706e-01,\n",
       "         9.67813285e-02,   2.18851960e-01,   3.40746481e-01])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6188,)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss test:  0.671511588758\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  56   27    0    0    0   21    2   41   46]\n",
      " [   0 1438  138    5    8    6   10    5    2]\n",
      " [   0  565  217    3    0    1   10    3    1]\n",
      " [   0  168   29   45    4   19    4    0    0]\n",
      " [   0   15    2    0  257    0    0    0    0]\n",
      " [   3   34    1    3    0 1301   23   29   20]\n",
      " [   2   55   20    1    0   30  159   14    3]\n",
      " [  10   16    2    0    0   27   10  772    9]\n",
      " [   5   26    0    1    0   15    1   19  429]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.74      0.29      0.42       193\n",
      "          2       0.61      0.89      0.73      1612\n",
      "          3       0.53      0.27      0.36       800\n",
      "          4       0.78      0.17      0.28       269\n",
      "          5       0.96      0.94      0.95       274\n",
      "          6       0.92      0.92      0.92      1414\n",
      "          7       0.73      0.56      0.63       284\n",
      "          8       0.87      0.91      0.89       846\n",
      "          9       0.84      0.86      0.85       496\n",
      "\n",
      "avg / total       0.76      0.76      0.73      6188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Log loss test: \",log_loss(y_test, y_pred_probs))\n",
    "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_test, y_preds))\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test, y_preds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553329023917259"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
