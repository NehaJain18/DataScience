{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report - Otto Group product classification\n",
    "\n",
    "[Introduction](#section1)\n",
    "\n",
    "[Client](#section2)\n",
    "\n",
    "[Data and its acquisition](#section3)\n",
    "\n",
    "[Data Exploration](#section4)\n",
    "\n",
    "- [Cleaning](#section5)\n",
    "- [Is there class imbalance?](#section6)\n",
    "- [Visualizing the data](#section7)\n",
    "- [Are some features related to each other?](#section8)\n",
    "- [Are the correlations statistically significant?](#section9)\n",
    "- [Important features to predict the target product category](#section10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Introduction\n",
    "\n",
    "Imagine that you are one of the biggest e-commerce companies in the world, with subsidiaries in many countries. Millions of products are sold and thousands are added to the product line every day. A consistent analysis of the products become crucial. But due to diverse global infrastructure, many identical products get classified differently. Therefore the quality of the product analysis depends heavily on the ability to accurately cluster similar products. The better the classification the more insights can be generated about the product range.\n",
    "\n",
    "<a id='section2'></a>\n",
    "# Client\n",
    "\n",
    "Here the client is the Otto-group company, one among the top e-commerce companies. We hope to provide the client with the best of the algorithms that would classify the given products as accurately as possible.\n",
    "\n",
    "<a id='section3'></a>\n",
    "# Data and its acquisition\n",
    "\n",
    "The data is provided by the client in the form of csv file. There is some additional test data that can be used to evaluate and report the algorithm success. The main data file has around 61 thousand products. Each of the product has 93 associated features and the correct class (product category) out of the total 9 classes. The actual meaning of the features is not available. So we can treat them just as numbers. Similarly the 9 classes are just numbers without any meaningful name. The actual meaning of the features and the product categories might have helped us understand the problem better. But this might be the case is many other projects where there are some confidentiality restrictions.\n",
    "\n",
    "Loading the data to use for modeling is a very simple step. I am using the read_csv function provided by the python-pandas library. It directly loads the data into a pandas dataframe (table like structure) object and can be used for processing readily.\n",
    "\n",
    "<a id='section4'></a>\n",
    "# Data exploration\n",
    "\n",
    "<a id='section5'></a>\n",
    "### Cleaning\n",
    "\n",
    "The data provided doesn't have null values or any missing values. The product id field is of no significance and is removed. All the 93 features together identifies the unique product for our purposes and is sufficient. The product classes are in text form and needs to be converted to the numerical values, so as to be consumed by machine learning libraries. We simply number them from 1 to 9.\n",
    "\n",
    "So our data has 61 thousand rows with 93 feature columns named feat_1, feat_2 ...feat_93 and 1 target column having values from 1 to 9. Here is how it looks:\n",
    "\n",
    "<img src=\"df-head.png\">\n",
    "\n",
    "<a id='section6'></a>\n",
    "### Is there any class imbalance?\n",
    "\n",
    "If there are approximately equal number of rows for each target class we call it \"balanced classes\" and \"imbalanced classes\" otherwise. We need to take different approaches in the both the cases. In general, the classes with more data will carry more weightage in our model unnecessarily, while the classes with less data would have negligible effect on our model. This is not what we want. We want the model to have equal representation from all the classes to do accurate predictions. Although imbalanced classes are not always a problem. Various algorithms deal with them internally. It also depends on how much the minority classes are important. Sometimes they are extremely important like in fraud detection cases. In such cases anomaly detection framework can also be used. In general there are few ways to deal with the imbalances classes:\n",
    "\n",
    "- While in the training phase use stratified sampling\n",
    "- Oversampling the minority classes\n",
    "- Undersampling the majority classes\n",
    "- At algorithm level, adjust the class weights or change the decision threshold\n",
    "- Don't use accuracy for the model evaluation, use AUC or F1-score instead.\n",
    "\n",
    "For the problem we are solving here, none of the class is more important than others. Also the class imbalance in the data might be natural i.e. there might be overall more products for certain classes. So we would got with stratified sampling in our model phase. Here is how the classes distribution look like:\n",
    "\n",
    "<img src=\"class-imbalance.png\" align=\"center\">\n",
    "\n",
    "\n",
    "The classes 2 and 6 have relatively more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='section7'></a>\n",
    "### Visualizing the data\n",
    "\n",
    "Since the data is huge simple reading the data doesn't help. Following visualizations will help understand the data better.\n",
    "\n",
    "##### Fig 1. Feature value counts\n",
    "\n",
    "- 93 features presented as different colors\n",
    "- The sharp jerky lines show that the feature values are integers\n",
    "- Most feature values are less than 80\n",
    "- Most feature values are conentrated in 0-5, hence log of value counts on y axis\n",
    "\n",
    "<img src=\"feature-counts-all.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Fig 2. Feature values and their target classes\n",
    "\n",
    "- Shows the range of the values each feature can take\n",
    "- Each feature has 9 separate rows for each target class\n",
    "- Two features with extreme values shown separately\n",
    "- the itensity of colors shows the concentration for that feature value\n",
    "\n",
    "<img src=\"all-feature-values.png\" align=\"left\">\n",
    "<img src=\"extreme-feature-values.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Fig 3. All 60K feature rows and their target classes\n",
    "\n",
    "- The width of the colored bars shows the class imbalances\n",
    "- Shows classes that have extreme feature data values\n",
    "- Features data overlap on the chart\n",
    "\n",
    "<font color='red'>should put title etc. </font>\n",
    "\n",
    "<img src=\"scatter-feature-vs-target.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "### Are some features related to each other?\n",
    "\n",
    "Knowing if some features are related to each other helps. If 2 features are linearly correlated one of them can be removed to produce a simpler ML model that defines relation between features and target.\n",
    "\n",
    "To find the correlation between 2 features we can find the pearson correlation index for them. If its close to 1 or -1, it would indicate strong linear relationship. But a value near 0 means no linear relation between the 2 features. We have 93 features and we need to find the correlation between all the pairs.\n",
    "\n",
    "##### Fig 1. Correlation Matrix\n",
    "\n",
    "- Features matrix\n",
    "- Colors bar represents the correlation index variance\n",
    "- Darker shaded squares have high correlation\n",
    "- Most high correlations are positive meaning the 2 feature values increase or decrease together\n",
    "\n",
    "<img src=\"correlation.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 2. Highest correlation feature pairs\n",
    "\n",
    "- [(8, 36), (39, 45), (3, 46), (3, 54), (9, 64), (15, 72), (29, 77), (30, 84)]\n",
    "\n",
    "<img src=\"corr-feat-pairs.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "### Are the correlations statistically significant?\n",
    "\n",
    "Formally, the correlation coefficient, r, tells us about the strength and direction of the linear relationship between x and y. However, the reliability of the linear model also depends on how many observed data points are in the sample. We need to look at both the value of the correlation coefficient r and the sample size n, together. The hypothesis test lets us decide whether the value of the population correlation coefficient ùõí is \"close to zero\" or \"significantly different from zero\". We decide this based on the sample correlation coefficient r and the sample size n.\n",
    "\n",
    "Null Hypothesis: H0: ùõí = 0\n",
    "\n",
    "Alternate Hypothesis: Ha: ùõí ‚â† 0\n",
    "\n",
    "**OR**\n",
    "\n",
    "Null Hypothesis H0: The population correlation coefficient IS NOT significantly different from zero. There IS NOT a significant linear relationship(correlation) between x and y in the population.\n",
    "\n",
    "Alternate Hypothesis Ha: The population correlation coefficient IS significantly DIFFERENT FROM zero. There IS A SIGNIFICANT LINEAR RELATIONSHIP (correlation) between x and y in the population.\n",
    "\n",
    "**at significance level (Œ± = 0.01)**\n",
    "\n",
    "We will accept or reject the Null hypothesis based on the p values. We will apply this to all the feature pairs and find the highest correlated features. And we can compare that with our previous results too.\n",
    "\n",
    "**Result: The p values were equal to 0 and we rejected the null hypothesis that the population correlation is zero. The pairs of features <font color=blue>[(8, 36), (39, 45), (3, 46), (3, 54), (9, 64), (15, 72), (29, 77), (30, 84)] </font>are statistically linearly correlated at significance level of 1%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "### Important features to predict the target product category\n",
    "\n",
    "For a given feature if the difference between the classes is significant then that feature is important and should be used while finding out the best model.\n",
    "\n",
    "To find out the intra class difference for each feature we can utilize ANOVA. ANOVA can determine whether the means of three or more groups are different. It uses F-tests to statistically test the equality of means. However before we jump in doing ANOVA, there are certain assumptions that should satisfy:\n",
    "\n",
    "- The different populations should have same variances. This is called assumption of homogeneity of variance\n",
    "- The different populations should be normally distributed\n",
    "- Samples are independent\n",
    "\n",
    "The 3rd assumption is apparently satisfied because there is no evidence in against of that. The first 2 assumptions can be relaxed a bit. Here is some visualizion of the data. The data spread is quite normal (with some skew) and similar spread/variance.\n",
    "\n",
    "##### Fig 1. Features data spread for each class (1 to 9)\n",
    "\n",
    "<img src=\"../data-storytelling/kde1.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde2.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde3.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde4.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde5.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde6.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde7.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde7.png\" align=\"left\">\n",
    "<img src=\"../data-storytelling/kde9.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANOVA and results\n",
    "\n",
    "After applying the ANOVA for each feature following features came up as the important features: \n",
    "<font color=\"blue\">feat_34, feat_11, feat_14, feat_60, feat_25</font>\n",
    "\n",
    "Less imortant features:\n",
    "<font color=\"blue\">feat_65, feat_6, feat_51, feat_63, feat_12</font>\n",
    "\n",
    "\n",
    "##### Fig. Kernel density plots for top features\n",
    "- features clearly differentiate between few classes\n",
    "- for example, class 5 data is striking different for feature 34\n",
    "\n",
    "<img src=\"top-feat-kdes.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig. Tukey plots to visualize further\n",
    "\n",
    "<img src=\"tukey.png\" align=\"left\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
